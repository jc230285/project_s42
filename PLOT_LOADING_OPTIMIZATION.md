# Plot Loading Performance Optimization Plan

## Current Performance Issues

### 1. **Full Reload on Every Change**
- When `selectedPlotIds` changes, ALL plots are re-fetched
- Comments for ALL plots are re-fetched
- Schema is re-fetched unnecessarily
- No caching or incremental loading

### 2. **No Plot Order Preservation**
- Plots are displayed in backend order, not selection order
- User expectation: plots appear in the order they were selected

### 3. **Bulk Loading Bottleneck**
```
Current: selectedPlotIds = [2, 24, 3, 1]
â†’ Backend fetches all 4 plots + all projects + all comments
â†’ Frontend re-renders everything
â†’ Total request: ~200-500KB depending on data
```

---

## Proposed Optimizations

### **Optimization 1: Incremental Plot Loading**

**Strategy**: Only fetch plots that aren't already cached

```typescript
interface PlotCache {
  [plotId: string]: {
    data: ProcessedPlot;
    project: ProcessedProject | null;
    comments: any[];
    loadedAt: number;
  }
}

const [plotCache, setPlotCache] = useState<PlotCache>({});

// Instead of full reload, load only missing plots
const loadMissingPlots = async (newPlotIds: string[]) => {
  const missingIds = newPlotIds.filter(id => !plotCache[id]);
  
  if (missingIds.length === 0) {
    // All plots already loaded, just reorder
    return;
  }
  
  // Fetch only missing plots
  const response = await fetch(`/projects/plots?plot_ids=${missingIds.join(',')}`);
  // ... merge into cache
};
```

**Benefits**:
- âœ… 80-90% reduction in data transfer when adding 1 plot to 10 existing
- âœ… Instant reordering (no refetch needed)
- âœ… Maintains scroll position

---

### **Optimization 2: Preserve Selection Order**

**Frontend Implementation**:
```typescript
// Current: selectedPlotIds is just an array
// Problem: Backend may return different order

// Solution: Track selection order with timestamp
interface PlotSelection {
  id: string;
  selectedAt: number;
}

const [plotSelections, setPlotSelections] = useState<PlotSelection[]>([]);

// When displaying, sort by selectedAt
const orderedPlots = plotSelections
  .sort((a, b) => a.selectedAt - b.selectedAt)
  .map(sel => plotCache[sel.id])
  .filter(Boolean);
```

**Backend Enhancement**:
```python
@app.get("/projects/plots")
def get_plots_data(
    plot_ids: str,
    preserve_order: bool = True  # New parameter
):
    # Parse: "2,24,3,1" â†’ [2, 24, 3, 1]
    requested_ids = [int(id) for id in plot_ids.split(',')]
    
    # Fetch plots
    plots = fetch_plots(requested_ids)
    
    if preserve_order:
        # Sort plots to match requested order
        id_to_plot = {p['_db_id']: p for p in plots}
        plots = [id_to_plot[id] for id in requested_ids if id in id_to_plot]
    
    return plots
```

**Benefits**:
- âœ… Plots appear in user's selection order
- âœ… Better UX for comparing multiple plots
- âœ… Predictable display order

---

### **Optimization 3: Lazy Load Comments**

**Current**: Comments loaded with every plot request

**Proposed**: Load comments on-demand per plot

```typescript
const [plotComments, setPlotComments] = useState<{[plotId: string]: any[]}>({});
const [loadingComments, setLoadingComments] = useState<Set<string>>(new Set());

// Load comments only when plot is expanded/viewed
const loadPlotComments = async (plotId: string) => {
  if (plotComments[plotId] || loadingComments.has(plotId)) return;
  
  setLoadingComments(prev => new Set(prev).add(plotId));
  
  const response = await fetch(`/plots/${plotId}/comments`);
  const comments = await response.json();
  
  setPlotComments(prev => ({ ...prev, [plotId]: comments }));
  setLoadingComments(prev => {
    const next = new Set(prev);
    next.delete(plotId);
    return next;
  });
};
```

**Backend**: New endpoint for individual plot comments
```python
@app.get("/plots/{plot_id}/comments")
def get_plot_comments(plot_id: int):
    # Return only comments for this specific plot
    pass
```

**Benefits**:
- âœ… 90% reduction in initial load time for plots list
- âœ… Comments load progressively as user scrolls
- âœ… Better for large comment datasets

---

### **Optimization 4: Smart Cache Invalidation**

```typescript
interface CacheEntry {
  data: any;
  loadedAt: number;
  lastModified?: number;
}

const CACHE_TTL = 5 * 60 * 1000; // 5 minutes

const isCacheValid = (entry: CacheEntry): boolean => {
  const age = Date.now() - entry.loadedAt;
  return age < CACHE_TTL;
};

// Invalidate cache when plot is updated
const handlePlotUpdate = (plotId: string) => {
  setPlotCache(prev => {
    const updated = { ...prev };
    delete updated[plotId]; // Force reload on next access
    return updated;
  });
};

// Refresh stale cache in background
const refreshStalePlots = async () => {
  const stalePlots = Object.entries(plotCache)
    .filter(([_, entry]) => !isCacheValid(entry))
    .map(([id]) => id);
    
  if (stalePlots.length > 0) {
    // Silent background refresh
    await loadPlots(stalePlots, { silent: true });
  }
};
```

---

### **Optimization 5: Request Batching & Debouncing**

```typescript
const [pendingPlotIds, setPendingPlotIds] = useState<Set<string>>(new Set());
const loadDebounceTimer = useRef<NodeJS.Timeout>();

const requestPlotLoad = (plotId: string) => {
  setPendingPlotIds(prev => new Set(prev).add(plotId));
  
  // Debounce: wait 300ms for more selections
  if (loadDebounceTimer.current) {
    clearTimeout(loadDebounceTimer.current);
  }
  
  loadDebounceTimer.current = setTimeout(() => {
    loadPlots(Array.from(pendingPlotIds));
    setPendingPlotIds(new Set());
  }, 300);
};
```

**Benefits**:
- âœ… Batch multiple rapid selections into single request
- âœ… Reduces server load
- âœ… Better UX during bulk selection

---

## Implementation Priority

### **Phase 1: Quick Wins** (1-2 hours)
1. âœ… Add `preserve_order` parameter to backend
2. âœ… Implement selection order tracking in frontend
3. âœ… Add request debouncing (300ms)

**Expected Impact**: 40% improvement in perceived performance

---

### **Phase 2: Caching** (2-3 hours)
1. âœ… Implement plot cache with TTL
2. âœ… Add incremental loading logic
3. âœ… Smart cache invalidation on updates

**Expected Impact**: 80% reduction in redundant data transfer

---

### **Phase 3: Lazy Loading** (2-4 hours)
1. âœ… Separate comments endpoint per plot
2. âœ… Load comments on plot expand
3. âœ… Add loading states

**Expected Impact**: 90% faster initial plot list load

---

## Performance Metrics

### Current Performance
```
Initial Load (4 plots):
- Request size: ~400KB
- Time: 2-3s
- Comments: All loaded upfront

Add 1 plot to existing 4:
- Request size: ~400KB (reloads all 5)
- Time: 2-3s
- Redundant data: 80%
```

### Expected After Optimization
```
Initial Load (4 plots):
- Request size: ~400KB (same)
- Time: 1-2s (backend order preserved)
- Comments: 0 (lazy loaded)

Add 1 plot to existing 4:
- Request size: ~80KB (only new plot)
- Time: 0.3-0.5s
- Redundant data: 0%
- Comments: Loaded on demand
```

---

## Code Changes Summary

### Frontend Changes
1. `frontend/app/projects/page.tsx`
   - Add `plotCache` state
   - Add `plotSelections` with timestamps
   - Implement incremental loading logic
   - Add debouncing for batch requests

### Backend Changes
1. `backend/app/main.py`
   - Add `preserve_order` parameter to `/projects/plots`
   - Create `/plots/{plot_id}/comments` endpoint
   - Add response caching headers

---

## Testing Plan

1. **Test Incremental Loading**
   - Select 1 plot â†’ verify 1 request
   - Select 4 more â†’ verify only 4 new plots fetched
   - Verify cached plots not refetched

2. **Test Selection Order**
   - Select plots: S001, S042, S003
   - Verify display order matches selection order
   - Deselect S042, reselect â†’ verify moves to end

3. **Test Comment Lazy Loading**
   - Load 10 plots
   - Verify no comment requests initially
   - Expand plot â†’ verify comment request
   - Collapse/expand â†’ verify cached (no new request)

4. **Test Performance**
   - Measure time for initial 10 plot load
   - Measure time for adding 11th plot
   - Verify < 500ms for single plot addition

---

## Monitoring & Metrics

Add performance tracking:
```typescript
// Track load times
performance.mark('plot-load-start');
await loadPlots(plotIds);
performance.mark('plot-load-end');
performance.measure('plot-load', 'plot-load-start', 'plot-load-end');

// Log to analytics
const duration = performance.getEntriesByName('plot-load')[0].duration;
console.log(`Plot load took ${duration}ms for ${plotIds.length} plots`);
```

---

## Risk Assessment

### Low Risk
- âœ… Selection order preservation (no breaking changes)
- âœ… Request debouncing (improves UX)

### Medium Risk  
- âš ï¸ Plot caching (requires cache invalidation strategy)
- âš ï¸ Incremental loading (complex state management)

### High Risk
- ðŸ”´ Lazy comment loading (major UX change, requires user testing)

---

## Rollback Plan

All optimizations should be feature-flagged:

```typescript
const ENABLE_PLOT_CACHE = process.env.NEXT_PUBLIC_ENABLE_PLOT_CACHE === 'true';
const ENABLE_LAZY_COMMENTS = process.env.NEXT_PUBLIC_ENABLE_LAZY_COMMENTS === 'true';

if (ENABLE_PLOT_CACHE) {
  // Use cached loading
} else {
  // Use original full reload
}
```

This allows quick rollback if issues arise in production.
